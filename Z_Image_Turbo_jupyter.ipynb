{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Z-Image-jupyter/blob/main/Z_Image_Turbo_jupyter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/comfyanonymous/ComfyUI\n",
    "\n",
    "%cd /content/ComfyUI\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d /content/ComfyUI/models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors -d /content/ComfyUI/models/clip -o qwen_3_4b.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors -d /content/ComfyUI/models/vae -o ae.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ComfyUI\n",
    "\n",
    "import os, random, time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
    "CLIPLoader = NODE_CLASS_MAPPINGS[\"CLIPLoader\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "KSampler = NODE_CLASS_MAPPINGS[\"KSampler\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet = UNETLoader.load_unet(\"z-image-turbo-fp8-e4m3fn.safetensors\", \"fp8_e4m3fn_fast\")[0]\n",
    "    clip = CLIPLoader.load_clip(\"qwen_3_4b.safetensors\", type=\"lumina2\")[0]\n",
    "    vae = VAELoader.load_vae(\"ae.safetensors\")[0]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(input):\n",
    "    tmp_dir=\"/content/ComfyUI/output\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "    values = input[\"input\"]\n",
    "\n",
    "    positive_prompt = values['positive_prompt']\n",
    "    negative_prompt = values['negative_prompt']\n",
    "    seed = values['seed'] # 0\n",
    "    steps = values['steps'] # 9\n",
    "    cfg = values['cfg'] # 1.0\n",
    "    sampler_name = values['sampler_name'] # euler\n",
    "    scheduler = values['scheduler'] # simple\n",
    "    denoise = values['denoise'] # 1.0\n",
    "    width = values['width'] # 1024\n",
    "    height = values['height'] # 1024\n",
    "    batch_size = values['batch_size'] # 1.0\n",
    "\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "\n",
    "    positive = CLIPTextEncode.encode(clip, positive_prompt)[0]\n",
    "    negative = CLIPTextEncode.encode(clip, negative_prompt)[0]\n",
    "    latent_image = EmptyLatentImage.generate(width, height, batch_size=batch_size)[0]\n",
    "    samples = KSampler.sample(unet, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)[0]\n",
    "    decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
    "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"{tmp_dir}/z_image_turbo.png\")\n",
    "\n",
    "    result = f\"{tmp_dir}/z_image_turbo.png\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "  \"input\": {\n",
    "    \"positive_prompt\": \"一位男士和他的贵宾犬穿着配套的服装参加狗狗秀，室内灯光，背景中有观众。\",\n",
    "    \"negative_prompt\": \"blurry ugly bad\",\n",
    "    \"width\": 1024,\n",
    "    \"height\": 1024,\n",
    "    \"batch_size\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"steps\": 9,\n",
    "    \"cfg\": 1,\n",
    "    \"sampler_name\": \"euler\",\n",
    "    \"scheduler\": \"simple\",\n",
    "    \"denoise\": 1.0,\n",
    "  }\n",
    "}\n",
    "\n",
    "output = generate(input)\n",
    "Image.open(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
