{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title <b>Time Out Preventer (Advanced)</b>\n",
        "%%capture\n",
        "AUTO_RECONNECT = True #@param {type:\"boolean\"}\n",
        "#@markdown **Run this code to prevent Google Colab from Timeout**\n",
        "from os import makedirs\n",
        "makedirs(\"/root/.config/rclone\", exist_ok = True)\n",
        "if AUTO_RECONNECT:\n",
        "  import IPython\n",
        "  from google.colab import output\n",
        "\n",
        "  display(IPython.display.Javascript('''\n",
        "  function ClickConnect(){\n",
        "    btn = document.querySelector(\"colab-connect-button\")\n",
        "    if (btn != null){\n",
        "      console.log(\"Click colab-connect-button\"); \n",
        "      btn.click() \n",
        "      }\n",
        "    \n",
        "    btn = document.getElementById('ok')\n",
        "    if (btn != null){\n",
        "      console.log(\"Click reconnect\"); \n",
        "      btn.click() \n",
        "      }\n",
        "    }\n",
        "\n",
        "  setInterval(ClickConnect,60000)\n",
        "  '''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd /content/ComfyUI\n",
        "!pip install -r requirements.txt\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors -d models/clip -o qwen_3_4b.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors -d models/vae -o ae.safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/ComfyUI\n",
        "import sys\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "import os, random, time, math\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPU is required for this notebook\"\n",
        "\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "UNETLoader = NODE_CLASS_MAPPINGS['UNETLoader']()\n",
        "CLIPLoader = NODE_CLASS_MAPPINGS['CLIPLoader']()\n",
        "VAELoader = NODE_CLASS_MAPPINGS['VAELoader']()\n",
        "CLIPTextEncode = NODE_CLASS_MAPPINGS['CLIPTextEncode']()\n",
        "KSampler = NODE_CLASS_MAPPINGS['KSampler']()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS['VAEDecode']()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS['EmptyLatentImage']()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    unet = UNETLoader.load_unet('z-image-turbo-fp8-e4m3fn.safetensors', 'fp8_e4m3fn_fast')[0]\n",
        "    clip = CLIPLoader.load_clip('qwen_3_4b.safetensors', type='lumina2')[0]\n",
        "    vae = VAELoader.load_vae('ae.safetensors')[0]\n",
        "\n",
        "PRESETS = {\n",
        "    'Cinematic HQ': 'cinematic lighting, ultra realistic, 8k detail, film still, shallow depth of field, dramatic contrast, sharp focus, global illumination',\n",
        "    'Portrait HQ': 'studio portrait, ultra realistic skin texture, subsurface scattering, soft rim light, high detail face, sharp focus',\n",
        "    'Anime HQ': 'anime style, extremely detailed illustration, clean lineart, vibrant colors, high resolution, sharp shading',\n",
        "    'Product HQ': 'product photography, ultra sharp focus, studio lighting, commercial quality, global illumination, clean background',\n",
        "    'NSFW Artistic HQ': 'artistic nude, sensual pose, fine art photography, soft cinematic lighting, detailed skin texture, aesthetic composition'\n",
        "}\n",
        "\n",
        "BASE_NEG = 'blurry, low quality, jpeg artifacts, bad anatomy, deformed, poorly drawn, oversaturated'\n",
        "SAFE_NEG = 'explicit sexual acts, porn, extreme fetish, gore'\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(v):\n",
        "    out = '/content/ComfyUI/output'\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "\n",
        "    prompt = f\"{PRESETS[v['preset']]}, {v['prompt']}\"\n",
        "    negative = BASE_NEG if v['nsfw'] else f\"{BASE_NEG}, {SAFE_NEG}\"\n",
        "\n",
        "    seed = v['seed'] if v['seed'] != 0 else random.randint(0, 2**64-1)\n",
        "\n",
        "    pos = CLIPTextEncode.encode(clip, prompt)[0]\n",
        "    neg = CLIPTextEncode.encode(clip, negative)[0]\n",
        "\n",
        "    latent = EmptyLatentImage.generate(v['width'], v['height'], batch_size=v['batch'])[0]\n",
        "    samples = KSampler.sample(\n",
        "        unet, seed,\n",
        "        v['steps'], v['cfg'],\n",
        "        v['sampler'], v['scheduler'],\n",
        "        pos, neg, latent,\n",
        "        denoise=v['denoise']\n",
        "    )[0]\n",
        "\n",
        "    decoded = VAEDecode.decode(vae, samples)[0]\n",
        "    imgs = []\n",
        "\n",
        "    for i in range(v['batch']):\n",
        "        img_array = decoded[i].cpu().numpy()\n",
        "        img_array = np.clip(img_array * 255, 0, 255).astype('uint8')\n",
        "        img = Image.fromarray(img_array)\n",
        "        imgs.append(img)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "def grid(imgs, cols):\n",
        "    rows = math.ceil(len(imgs)/cols)\n",
        "    w, h = imgs[0].size\n",
        "    g = Image.new('RGB', (cols*w, rows*h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        g.paste(img, ((i%cols)*w, (i//cols)*h))\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets as w\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "preset = w.Dropdown(options=list(PRESETS.keys()), value='Cinematic HQ', description='Preset')\n",
        "nsfw = w.Checkbox(value=False, description='NSFW')\n",
        "prompt = w.Textarea(value='a woman standing in soft window light, ultra detailed', layout=w.Layout(width='650px', height='80px'))\n",
        "\n",
        "width = w.IntSlider(value=1024, min=512, max=2048, step=64, description='Width')\n",
        "height = w.IntSlider(value=1024, min=512, max=2048, step=64, description='Height')\n",
        "steps = w.IntSlider(value=18, min=6, max=40, step=1, description='Steps')\n",
        "cfg = w.FloatSlider(value=2.0, min=0.5, max=6.0, step=0.1, description='CFG')\n",
        "denoise = w.FloatSlider(value=1.0, min=0.6, max=1.0, step=0.05, description='Denoise')\n",
        "batch = w.IntSlider(value=1, min=1, max=4, step=1, description='Batch')\n",
        "seed = w.IntText(value=0, description='Seed')\n",
        "\n",
        "sampler = w.Dropdown(options=['dpmpp_2m', 'euler', 'euler_ancestral', 'dpm_fast'], value='dpmpp_2m', description='Sampler')\n",
        "scheduler = w.Dropdown(options=['karras', 'simple'], value='karras', description='Scheduler')\n",
        "\n",
        "use_grid = w.Checkbox(value=True, description='Grid')\n",
        "cols = w.IntSlider(value=2, min=2, max=4, step=1, description='Cols')\n",
        "\n",
        "btn = w.Button(description='GENERATE HQ', button_style='success')\n",
        "out = w.Output()\n",
        "\n",
        "def run(b):\n",
        "    with out:\n",
        "        clear_output(wait=True)\n",
        "        v = dict(\n",
        "            preset=preset.value,\n",
        "            nsfw=nsfw.value,\n",
        "            prompt=prompt.value,\n",
        "            width=width.value,\n",
        "            height=height.value,\n",
        "            steps=steps.value,\n",
        "            cfg=cfg.value,\n",
        "            denoise=denoise.value,\n",
        "            batch=batch.value,\n",
        "            seed=seed.value,\n",
        "            sampler=sampler.value,\n",
        "            scheduler=scheduler.value\n",
        "        )\n",
        "        try:\n",
        "            imgs = generate(v)\n",
        "            if use_grid.value and len(imgs) > 1:\n",
        "                display(grid(imgs, cols.value))\n",
        "            else:\n",
        "                for i in imgs:\n",
        "                    display(i)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "btn.on_click(run)\n",
        "\n",
        "display(w.VBox([\n",
        "    preset, nsfw, prompt,\n",
        "    width, height,\n",
        "    steps, cfg, denoise,\n",
        "    batch, seed,\n",
        "    sampler, scheduler,\n",
        "    use_grid, cols,\n",
        "    btn, out\n",
        "]))"
      ]
    }
  ]
}
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors -d models/diffusion_models -o z-image-turbo-fp8-e4m3fn.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors -d models/clip -o qwen_3_4b.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors -d models/vae -o ae.safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/ComfyUI\n",
        "import sys\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "import os, random, time, math\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "assert torch.cuda.is_available(), \"GPU is required for this notebook\"\n",
        "\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "UNETLoader = NODE_CLASS_MAPPINGS['UNETLoader']()\n",
        "CLIPLoader = NODE_CLASS_MAPPINGS['CLIPLoader']()\n",
        "VAELoader = NODE_CLASS_MAPPINGS['VAELoader']()\n",
        "CLIPTextEncode = NODE_CLASS_MAPPINGS['CLIPTextEncode']()\n",
        "KSampler = NODE_CLASS_MAPPINGS['KSampler']()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS['VAEDecode']()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS['EmptyLatentImage']()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    unet = UNETLoader.load_unet('z-image-turbo-fp8-e4m3fn.safetensors', 'fp8_e4m3fn_fast')[0]\n",
        "    clip = CLIPLoader.load_clip('qwen_3_4b.safetensors', type='lumina2')[0]\n",
        "    vae = VAELoader.load_vae('ae.safetensors')[0]\n",
        "\n",
        "PRESETS = {\n",
        "    'Cinematic HQ': 'cinematic lighting, ultra realistic, 8k detail, film still, shallow depth of field, dramatic contrast, sharp focus, global illumination',\n",
        "    'Portrait HQ': 'studio portrait, ultra realistic skin texture, subsurface scattering, soft rim light, high detail face, sharp focus',\n",
        "    'Anime HQ': 'anime style, extremely detailed illustration, clean lineart, vibrant colors, high resolution, sharp shading',\n",
        "    'Product HQ': 'product photography, ultra sharp focus, studio lighting, commercial quality, global illumination, clean background',\n",
        "    'NSFW Artistic HQ': 'artistic nude, sensual pose, fine art photography, soft cinematic lighting, detailed skin texture, aesthetic composition'\n",
        "}\n",
        "\n",
        "BASE_NEG = 'blurry, low quality, jpeg artifacts, bad anatomy, deformed, poorly drawn, oversaturated'\n",
        "SAFE_NEG = 'explicit sexual acts, porn, extreme fetish, gore'\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(v):\n",
        "    out = '/content/ComfyUI/output'\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "\n",
        "    prompt = f\"{PRESETS[v['preset']]}, {v['prompt']}\"\n",
        "    negative = BASE_NEG if v['nsfw'] else f\"{BASE_NEG}, {SAFE_NEG}\"\n",
        "\n",
        "    seed = v['seed'] if v['seed'] != 0 else random.randint(0, 2**64-1)\n",
        "\n",
        "    pos = CLIPTextEncode.encode(clip, prompt)[0]\n",
        "    neg = CLIPTextEncode.encode(clip, negative)[0]\n",
        "\n",
        "    latent = EmptyLatentImage.generate(v['width'], v['height'], batch_size=v['batch'])[0]\n",
        "    samples = KSampler.sample(\n",
        "        unet, seed,\n",
        "        v['steps'], v['cfg'],\n",
        "        v['sampler'], v['scheduler'],\n",
        "        pos, neg, latent,\n",
        "        denoise=v['denoise']\n",
        "    )[0]\n",
        "\n",
        "    decoded = VAEDecode.decode(vae, samples)[0]\n",
        "    imgs = []\n",
        "\n",
        "    for i in range(v['batch']):\n",
        "        img_array = decoded[i].cpu().numpy()\n",
        "        img_array = np.clip(img_array * 255, 0, 255).astype('uint8')\n",
        "        img = Image.fromarray(img_array)\n",
        "        imgs.append(img)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "def grid(imgs, cols):\n",
        "    rows = math.ceil(len(imgs)/cols)\n",
        "    w, h = imgs[0].size\n",
        "    g = Image.new('RGB', (cols*w, rows*h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        g.paste(img, ((i%cols)*w, (i//cols)*h))\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipywidgets as w\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "preset = w.Dropdown(options=list(PRESETS.keys()), value='Cinematic HQ', description='Preset')\n",
        "nsfw = w.Checkbox(value=False, description='NSFW')\n",
        "prompt = w.Textarea(value='a woman standing in soft window light, ultra detailed', layout=w.Layout(width='650px', height='80px'))\n",
        "\n",
        "width = w.IntSlider(value=1024, min=512, max=2048, step=64, description='Width')\n",
        "height = w.IntSlider(value=1024, min=512, max=2048, step=64, description='Height')\n",
        "steps = w.IntSlider(value=18, min=6, max=40, step=1, description='Steps')\n",
        "cfg = w.FloatSlider(value=2.0, min=0.5, max=6.0, step=0.1, description='CFG')\n",
        "denoise = w.FloatSlider(value=1.0, min=0.6, max=1.0, step=0.05, description='Denoise')\n",
        "batch = w.IntSlider(value=1, min=1, max=4, step=1, description='Batch')\n",
        "seed = w.IntText(value=0, description='Seed')\n",
        "\n",
        "sampler = w.Dropdown(options=['dpmpp_2m', 'euler', 'euler_ancestral', 'dpm_fast'], value='dpmpp_2m', description='Sampler')\n",
        "scheduler = w.Dropdown(options=['karras', 'simple'], value='karras', description='Scheduler')\n",
        "\n",
        "use_grid = w.Checkbox(value=True, description='Grid')\n",
        "cols = w.IntSlider(value=2, min=2, max=4, step=1, description='Cols')\n",
        "\n",
        "btn = w.Button(description='GENERATE HQ', button_style='success')\n",
        "out = w.Output()\n",
        "\n",
        "def run(b):\n",
        "    with out:\n",
        "        clear_output(wait=True)\n",
        "        v = dict(\n",
        "            preset=preset.value,\n",
        "            nsfw=nsfw.value,\n",
        "            prompt=prompt.value,\n",
        "            width=width.value,\n",
        "            height=height.value,\n",
        "            steps=steps.value,\n",
        "            cfg=cfg.value,\n",
        "            denoise=denoise.value,\n",
        "            batch=batch.value,\n",
        "            seed=seed.value,\n",
        "            sampler=sampler.value,\n",
        "            scheduler=scheduler.value\n",
        "        )\n",
        "        try:\n",
        "            imgs = generate(v)\n",
        "            if use_grid.value and len(imgs) > 1:\n",
        "                display(grid(imgs, cols.value))\n",
        "            else:\n",
        "                for i in imgs:\n",
        "                    display(i)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "btn.on_click(run)\n",
        "\n",
        "display(w.VBox([\n",
        "    preset, nsfw, prompt,\n",
        "    width, height,\n",
        "    steps, cfg, denoise,\n",
        "    batch, seed,\n",
        "    sampler, scheduler,\n",
        "    use_grid, cols,\n",
        "    btn, out\n",
        "]))"
      ]
    }
  ]
}
